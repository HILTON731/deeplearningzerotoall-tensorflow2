{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "         [2, 1, 3, 2],\n",
    "         [3, 1, 3, 4],\n",
    "         [4, 1, 5, 5],\n",
    "         [1, 7, 5, 5],\n",
    "         [1, 2, 5, 6],\n",
    "         [1, 6, 6, 6],\n",
    "         [1, 7, 7, 7]]\n",
    "\n",
    "# one-hot encoding\n",
    "y_data = [[0, 0, 1],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 1],\n",
    "         [0, 1, 0],\n",
    "         [0, 1, 0],\n",
    "         [0, 1, 0],\n",
    "         [1, 0, 0],\n",
    "         [1, 0, 0]]\n",
    "\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.19633393, -0.67894936,  0.07726288],\n",
      "       [-0.72127634, -0.9497583 , -2.4160798 ],\n",
      "       [ 0.2874941 , -1.1374302 , -0.03768032],\n",
      "       [ 0.82390803,  1.7027009 , -0.3149968 ]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([ 0.6632706 , -1.1701741 ,  0.90436906], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.0018742e-06 9.9999797e-01 5.0492748e-09]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_db = [[8, 2, 1, 4]]\n",
    "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(sample_db, W) + b)\n",
    "\n",
    "print(hypothesis) # Softmax를 이용한 one-hot encoding\n",
    "print(tf.reduce_sum(hypothesis)) # 각 확률치를 더한 값 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.6782494e-01 2.3646800e-02 8.5283229e-03]\n",
      " [9.9117821e-01 1.7668776e-03 7.0548435e-03]\n",
      " [9.9506915e-01 4.2863437e-03 6.4452097e-04]\n",
      " [9.9965394e-01 2.4999675e-04 9.6041374e-05]\n",
      " [9.9912351e-01 8.7644544e-04 5.2613958e-09]\n",
      " [9.9341488e-01 6.5770885e-03 8.0206755e-06]\n",
      " [9.9936193e-01 6.3808059e-04 6.6283721e-09]\n",
      " [9.9970585e-01 2.9418091e-04 2.8162558e-10]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "print(hypothesis(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.6780252, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    return cost_mean\n",
    "\n",
    "print(cost_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[ 1.4907014 , -0.74382186, -0.7468795 ],\n",
      "       [ 1.7370448 , -1.2401532 , -0.4968915 ],\n",
      "       [ 2.7352052 , -1.8642235 , -0.8709817 ],\n",
      "       [ 2.8598685 , -1.9880865 , -0.87178195]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.74316657, -0.37020802, -0.3729585 ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = cost_fn(X, Y)\n",
    "        grads = tape.gradient(cost, variables)\n",
    "        return grads\n",
    "print(grad_fn(x_data, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 4.342783451080322\n",
      "Loss at epoch 100: 0.9157935380935669\n",
      "Loss at epoch 200: 0.772110641002655\n",
      "Loss at epoch 300: 0.6907076835632324\n",
      "Loss at epoch 400: 0.6399660110473633\n",
      "Loss at epoch 500: 0.6062471270561218\n",
      "Loss at epoch 600: 0.5819138288497925\n",
      "Loss at epoch 700: 0.562882125377655\n",
      "Loss at epoch 800: 0.5470860004425049\n",
      "Loss at epoch 900: 0.5334579944610596\n",
      "Loss at epoch 1000: 0.5214025974273682\n",
      "Loss at epoch 1100: 0.510557234287262\n",
      "Loss at epoch 1200: 0.5006817579269409\n",
      "Loss at epoch 1300: 0.49160635471343994\n",
      "Loss at epoch 1400: 0.48320499062538147\n",
      "Loss at epoch 1500: 0.4753805100917816\n",
      "Loss at epoch 1600: 0.4680556356906891\n",
      "Loss at epoch 1700: 0.4611680507659912\n",
      "Loss at epoch 1800: 0.4546663463115692\n",
      "Loss at epoch 1900: 0.44850751757621765\n",
      "Loss at epoch 2000: 0.4426555633544922\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1) % verbose == 0):\n",
    "            print('Loss at epoch {}: {}'.format(i+1, cost_fn(X,Y).numpy()))\n",
    "\n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.03448453 0.08983088 0.87568456]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2, 1, 3, 2]]\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a)\n",
    "print(tf.argmax(a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02944318 0.05076008 0.91979676]\n",
      " [0.03448451 0.08983088 0.87568456]\n",
      " [0.00414131 0.4842729  0.51158583]\n",
      " [0.00367719 0.61517006 0.38115278]\n",
      " [0.57253414 0.39095262 0.03651316]\n",
      " [0.27990058 0.71060854 0.0094909 ]\n",
      " [0.6153006  0.3769333  0.00776609]\n",
      " [0.668747   0.329473   0.00177998]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 0 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 5.452994346618652\n",
      "Loss at epoch 100: 0.684638500213623\n",
      "Loss at epoch 200: 0.5929224491119385\n",
      "Loss at epoch 300: 0.5313973426818848\n",
      "Loss at epoch 400: 0.47873741388320923\n",
      "Loss at epoch 500: 0.429477334022522\n",
      "Loss at epoch 600: 0.3816629648208618\n",
      "Loss at epoch 700: 0.33455735445022583\n",
      "Loss at epoch 800: 0.2886422276496887\n",
      "Loss at epoch 900: 0.2505078911781311\n",
      "Loss at epoch 1000: 0.234241783618927\n",
      "Loss at epoch 1100: 0.22254188358783722\n",
      "Loss at epoch 1200: 0.2119368016719818\n",
      "Loss at epoch 1300: 0.20227447152137756\n",
      "Loss at epoch 1400: 0.1934325248003006\n",
      "Loss at epoch 1500: 0.18530979752540588\n",
      "Loss at epoch 1600: 0.17782223224639893\n",
      "Loss at epoch 1700: 0.17089824378490448\n",
      "Loss at epoch 1800: 0.16447734832763672\n",
      "Loss at epoch 1900: 0.1585073322057724\n",
      "Loss at epoch 2000: 0.15294276177883148\n"
     ]
    }
   ],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)\n",
    "            \n",
    "            return grads\n",
    "\n",
    "    def fit(self, X, Y, epochs=2000, verbose=100):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            grads = self.grad_fn(X, Y)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch {}: {}'.format(i+1, self.cost_fn(X, Y).numpy()))\n",
    "    \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
