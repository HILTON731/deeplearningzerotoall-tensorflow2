{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0396 Batch Loss 0.9890\n",
      "Epoch 10 Loss 0.0378 Batch Loss 0.9455\n",
      "Epoch 20 Loss 0.0343 Batch Loss 0.8585\n",
      "Epoch 30 Loss 0.0324 Batch Loss 0.8095\n",
      "Epoch 40 Loss 0.0293 Batch Loss 0.7327\n",
      "Epoch 50 Loss 0.0218 Batch Loss 0.5439\n",
      "Epoch 60 Loss 0.0156 Batch Loss 0.3903\n",
      "Epoch 70 Loss 0.0105 Batch Loss 0.2620\n",
      "Epoch 80 Loss 0.0066 Batch Loss 0.1652\n",
      "Epoch 90 Loss 0.0040 Batch Loss 0.0993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25f2f5b2148>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I feel hungry\n",
      "Predicted translation: 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
